{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7500\n",
      "Epoch [20/300], Test Accuracy: 0.7500\n",
      "Epoch [30/300], Test Accuracy: 0.7562\n",
      "Epoch [40/300], Test Accuracy: 0.7469\n",
      "Epoch [50/300], Test Accuracy: 0.7500\n",
      "Epoch [60/300], Test Accuracy: 0.7531\n",
      "Epoch [70/300], Test Accuracy: 0.7594\n",
      "Epoch [80/300], Test Accuracy: 0.7562\n",
      "Epoch [90/300], Test Accuracy: 0.7594\n",
      "Epoch [100/300], Test Accuracy: 0.7625\n",
      "Epoch [110/300], Test Accuracy: 0.7656\n",
      "Epoch [120/300], Test Accuracy: 0.7594\n",
      "Epoch [130/300], Test Accuracy: 0.7656\n",
      "Epoch [140/300], Test Accuracy: 0.7594\n",
      "Epoch [150/300], Test Accuracy: 0.7656\n",
      "Epoch [160/300], Test Accuracy: 0.7656\n",
      "Epoch [170/300], Test Accuracy: 0.7625\n",
      "Epoch [180/300], Test Accuracy: 0.7594\n",
      "Epoch [190/300], Test Accuracy: 0.7656\n",
      "Epoch [200/300], Test Accuracy: 0.7625\n",
      "Epoch [210/300], Test Accuracy: 0.7562\n",
      "Epoch [220/300], Test Accuracy: 0.7625\n",
      "Epoch [230/300], Test Accuracy: 0.7656\n",
      "Epoch [240/300], Test Accuracy: 0.7656\n",
      "Epoch [250/300], Test Accuracy: 0.7625\n",
      "Epoch [260/300], Test Accuracy: 0.7625\n",
      "Epoch [270/300], Test Accuracy: 0.7625\n",
      "Epoch [280/300], Test Accuracy: 0.7656\n",
      "Epoch [290/300], Test Accuracy: 0.7656\n",
      "Epoch [300/300], Test Accuracy: 0.7656\n",
      "Fold 1 Accuracy: 0.7773\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7500\n",
      "Epoch [20/300], Test Accuracy: 0.7438\n",
      "Epoch [30/300], Test Accuracy: 0.7406\n",
      "Epoch [40/300], Test Accuracy: 0.7500\n",
      "Epoch [50/300], Test Accuracy: 0.7562\n",
      "Epoch [60/300], Test Accuracy: 0.7562\n",
      "Epoch [70/300], Test Accuracy: 0.7625\n",
      "Epoch [80/300], Test Accuracy: 0.7625\n",
      "Epoch [90/300], Test Accuracy: 0.7656\n",
      "Epoch [100/300], Test Accuracy: 0.7656\n",
      "Epoch [110/300], Test Accuracy: 0.7656\n",
      "Epoch [120/300], Test Accuracy: 0.7594\n",
      "Epoch [130/300], Test Accuracy: 0.7594\n",
      "Epoch [140/300], Test Accuracy: 0.7625\n",
      "Epoch [150/300], Test Accuracy: 0.7625\n",
      "Epoch [160/300], Test Accuracy: 0.7656\n",
      "Epoch [170/300], Test Accuracy: 0.7656\n",
      "Epoch [180/300], Test Accuracy: 0.7656\n",
      "Epoch [190/300], Test Accuracy: 0.7625\n",
      "Epoch [200/300], Test Accuracy: 0.7625\n",
      "Epoch [210/300], Test Accuracy: 0.7625\n",
      "Epoch [220/300], Test Accuracy: 0.7656\n",
      "Epoch [230/300], Test Accuracy: 0.7688\n",
      "Epoch [240/300], Test Accuracy: 0.7625\n",
      "Epoch [250/300], Test Accuracy: 0.7656\n",
      "Epoch [260/300], Test Accuracy: 0.7625\n",
      "Epoch [270/300], Test Accuracy: 0.7656\n",
      "Epoch [280/300], Test Accuracy: 0.7656\n",
      "Epoch [290/300], Test Accuracy: 0.7625\n",
      "Epoch [300/300], Test Accuracy: 0.7625\n",
      "Fold 2 Accuracy: 0.7617\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7406\n",
      "Epoch [20/300], Test Accuracy: 0.7344\n",
      "Epoch [30/300], Test Accuracy: 0.7500\n",
      "Epoch [40/300], Test Accuracy: 0.7344\n",
      "Epoch [50/300], Test Accuracy: 0.7375\n",
      "Epoch [60/300], Test Accuracy: 0.7438\n",
      "Epoch [70/300], Test Accuracy: 0.7594\n",
      "Epoch [80/300], Test Accuracy: 0.7562\n",
      "Epoch [90/300], Test Accuracy: 0.7594\n",
      "Epoch [100/300], Test Accuracy: 0.7625\n",
      "Epoch [110/300], Test Accuracy: 0.7656\n",
      "Epoch [120/300], Test Accuracy: 0.7594\n",
      "Epoch [130/300], Test Accuracy: 0.7625\n",
      "Epoch [140/300], Test Accuracy: 0.7656\n",
      "Epoch [150/300], Test Accuracy: 0.7625\n",
      "Epoch [160/300], Test Accuracy: 0.7562\n",
      "Epoch [170/300], Test Accuracy: 0.7625\n",
      "Epoch [180/300], Test Accuracy: 0.7656\n",
      "Epoch [190/300], Test Accuracy: 0.7625\n",
      "Epoch [200/300], Test Accuracy: 0.7625\n",
      "Epoch [210/300], Test Accuracy: 0.7625\n",
      "Epoch [220/300], Test Accuracy: 0.7625\n",
      "Epoch [230/300], Test Accuracy: 0.7656\n",
      "Epoch [240/300], Test Accuracy: 0.7625\n",
      "Epoch [250/300], Test Accuracy: 0.7656\n",
      "Epoch [260/300], Test Accuracy: 0.7594\n",
      "Epoch [270/300], Test Accuracy: 0.7656\n",
      "Epoch [280/300], Test Accuracy: 0.7594\n",
      "Epoch [290/300], Test Accuracy: 0.7656\n",
      "Epoch [300/300], Test Accuracy: 0.7625\n",
      "Fold 3 Accuracy: 0.7539\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7500\n",
      "Epoch [20/300], Test Accuracy: 0.7500\n",
      "Epoch [30/300], Test Accuracy: 0.7562\n",
      "Epoch [40/300], Test Accuracy: 0.7531\n",
      "Epoch [50/300], Test Accuracy: 0.7625\n",
      "Epoch [60/300], Test Accuracy: 0.7812\n",
      "Epoch [70/300], Test Accuracy: 0.7906\n",
      "Epoch [80/300], Test Accuracy: 0.7719\n",
      "Epoch [90/300], Test Accuracy: 0.7844\n",
      "Epoch [100/300], Test Accuracy: 0.7875\n",
      "Epoch [110/300], Test Accuracy: 0.7875\n",
      "Epoch [120/300], Test Accuracy: 0.7812\n",
      "Epoch [130/300], Test Accuracy: 0.7844\n",
      "Epoch [140/300], Test Accuracy: 0.7844\n",
      "Epoch [150/300], Test Accuracy: 0.7844\n",
      "Epoch [160/300], Test Accuracy: 0.7875\n",
      "Epoch [170/300], Test Accuracy: 0.7844\n",
      "Epoch [180/300], Test Accuracy: 0.7875\n",
      "Epoch [190/300], Test Accuracy: 0.7875\n",
      "Epoch [200/300], Test Accuracy: 0.7875\n",
      "Epoch [210/300], Test Accuracy: 0.7875\n",
      "Epoch [220/300], Test Accuracy: 0.7875\n",
      "Epoch [230/300], Test Accuracy: 0.7875\n",
      "Epoch [240/300], Test Accuracy: 0.7844\n",
      "Epoch [250/300], Test Accuracy: 0.7812\n",
      "Epoch [260/300], Test Accuracy: 0.7937\n",
      "Epoch [270/300], Test Accuracy: 0.7812\n",
      "Epoch [280/300], Test Accuracy: 0.7875\n",
      "Epoch [290/300], Test Accuracy: 0.7875\n",
      "Epoch [300/300], Test Accuracy: 0.7844\n",
      "Fold 4 Accuracy: 0.7812\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7594\n",
      "Epoch [20/300], Test Accuracy: 0.7469\n",
      "Epoch [30/300], Test Accuracy: 0.7500\n",
      "Epoch [40/300], Test Accuracy: 0.7500\n",
      "Epoch [50/300], Test Accuracy: 0.7562\n",
      "Epoch [60/300], Test Accuracy: 0.7531\n",
      "Epoch [70/300], Test Accuracy: 0.7562\n",
      "Epoch [80/300], Test Accuracy: 0.7531\n",
      "Epoch [90/300], Test Accuracy: 0.7562\n",
      "Epoch [100/300], Test Accuracy: 0.7500\n",
      "Epoch [110/300], Test Accuracy: 0.7531\n",
      "Epoch [120/300], Test Accuracy: 0.7531\n",
      "Epoch [130/300], Test Accuracy: 0.7594\n",
      "Epoch [140/300], Test Accuracy: 0.7500\n",
      "Epoch [150/300], Test Accuracy: 0.7500\n",
      "Epoch [160/300], Test Accuracy: 0.7531\n",
      "Epoch [170/300], Test Accuracy: 0.7531\n",
      "Epoch [180/300], Test Accuracy: 0.7500\n",
      "Epoch [190/300], Test Accuracy: 0.7562\n",
      "Epoch [200/300], Test Accuracy: 0.7531\n",
      "Epoch [210/300], Test Accuracy: 0.7531\n",
      "Epoch [220/300], Test Accuracy: 0.7562\n",
      "Epoch [230/300], Test Accuracy: 0.7562\n",
      "Epoch [240/300], Test Accuracy: 0.7531\n",
      "Epoch [250/300], Test Accuracy: 0.7500\n",
      "Epoch [260/300], Test Accuracy: 0.7562\n",
      "Epoch [270/300], Test Accuracy: 0.7562\n",
      "Epoch [280/300], Test Accuracy: 0.7469\n",
      "Epoch [290/300], Test Accuracy: 0.7531\n",
      "Epoch [300/300], Test Accuracy: 0.7531\n",
      "Fold 5 Accuracy: 0.7020\n",
      "Mean CV Accuracy: 0.7552 (+/- 0.0285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thoma\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Test Accuracy: 0.7438\n",
      "Epoch [20/300], Test Accuracy: 0.7469\n",
      "Epoch [30/300], Test Accuracy: 0.7500\n",
      "Epoch [40/300], Test Accuracy: 0.7719\n",
      "Epoch [50/300], Test Accuracy: 0.7562\n",
      "Epoch [60/300], Test Accuracy: 0.7688\n",
      "Epoch [70/300], Test Accuracy: 0.7656\n",
      "Epoch [80/300], Test Accuracy: 0.7688\n",
      "Epoch [90/300], Test Accuracy: 0.7688\n",
      "Epoch [100/300], Test Accuracy: 0.7719\n",
      "Epoch [110/300], Test Accuracy: 0.7688\n",
      "Epoch [120/300], Test Accuracy: 0.7656\n",
      "Epoch [130/300], Test Accuracy: 0.7625\n",
      "Epoch [140/300], Test Accuracy: 0.7688\n",
      "Epoch [150/300], Test Accuracy: 0.7688\n",
      "Epoch [160/300], Test Accuracy: 0.7688\n",
      "Epoch [170/300], Test Accuracy: 0.7688\n",
      "Epoch [180/300], Test Accuracy: 0.7656\n",
      "Epoch [190/300], Test Accuracy: 0.7719\n",
      "Epoch [200/300], Test Accuracy: 0.7750\n",
      "Epoch [210/300], Test Accuracy: 0.7656\n",
      "Epoch [220/300], Test Accuracy: 0.7656\n",
      "Epoch [230/300], Test Accuracy: 0.7719\n",
      "Epoch [240/300], Test Accuracy: 0.7719\n",
      "Epoch [250/300], Test Accuracy: 0.7688\n",
      "Epoch [260/300], Test Accuracy: 0.7719\n",
      "Epoch [270/300], Test Accuracy: 0.7750\n",
      "Epoch [280/300], Test Accuracy: 0.7719\n",
      "Epoch [290/300], Test Accuracy: 0.7688\n",
      "Epoch [300/300], Test Accuracy: 0.7719\n",
      "KNN Accuracy: 0.7219\n",
      "Random Forest Accuracy: 0.7937\n",
      "Gradient Boosting Accuracy: 0.7812\n",
      "Neural Network Accuracy: 0.7719\n",
      "Ensemble Accuracy: 0.7844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "y = (y >= 6).astype(int)\n",
    "\n",
    "# Feature engineering\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class ImprovedAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim, hidden_dim, num_classes, dropout_rate=0.5):\n",
    "        super(ImprovedAttentionModel, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_weights = F.softmax(self.attention(x), dim=1)\n",
    "        weighted_features = x * attention_weights\n",
    "        x = self.dropout(F.leaky_relu(self.bn1(self.fc1(weighted_features))))\n",
    "        x = self.dropout(F.leaky_relu(self.bn2(self.fc2(x))))\n",
    "        output = self.fc3(x)\n",
    "        return output, weighted_features, attention_weights\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=200):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _, _ = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs, _, _ = model(X_test_tensor)\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "attention_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 300\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    X_train_fold = X_train_tensor[train_idx]\n",
    "    y_train_fold = y_train_tensor[train_idx]\n",
    "    X_val_fold = X_train_tensor[val_idx]\n",
    "    y_val_fold = y_train_tensor[val_idx]\n",
    "    \n",
    "    model = ImprovedAttentionModel(input_dim, attention_dim, hidden_dim, num_classes, dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs, _, _ = model(X_val_fold)\n",
    "        _, predicted = torch.max(val_outputs.data, 1)\n",
    "        accuracy = accuracy_score(y_val_fold.cpu(), predicted.cpu())\n",
    "        cv_scores.append(accuracy)\n",
    "        print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "final_model = ImprovedAttentionModel(input_dim, attention_dim, hidden_dim, num_classes, dropout_rate).to(device)\n",
    "final_criterion = nn.CrossEntropyLoss()\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "final_scheduler = ReduceLROnPlateau(final_optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "\n",
    "final_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "final_model = train_model(final_model, final_train_loader, final_criterion, final_optimizer, final_scheduler, num_epochs)\n",
    "\n",
    "# Extract attention-weighted features\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    _, train_features, _ = final_model(X_train_tensor)\n",
    "    _, test_features, _ = final_model(X_test_tensor)\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "test_features = test_features.cpu().numpy()\n",
    "\n",
    "# Create an ensemble of multiple models\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "knn.fit(train_features, y_train)\n",
    "rf.fit(train_features, y_train)\n",
    "gb.fit(train_features, y_train)\n",
    "\n",
    "knn_predictions = knn.predict(test_features)\n",
    "rf_predictions = rf.predict(test_features)\n",
    "gb_predictions = gb.predict(test_features)\n",
    "nn_predictions = final_model(X_test_tensor)[0].argmax(dim=1).cpu().numpy()\n",
    "\n",
    "ensemble_predictions = np.round((knn_predictions + rf_predictions + gb_predictions + nn_predictions) / 4).astype(int)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_predictions):.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_predictions):.4f}\")\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_predictions):.4f}\")\n",
    "print(f\"Neural Network Accuracy: {accuracy_score(y_test, nn_predictions):.4f}\")\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#Set up modern seaborn styling\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)  # Modern way to set seaborn style\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Define color palette\n",
    "colors = ['#8ECFC9', '#FFBE7A', '#FA7F6F', '#82B0D2', '#BEB8DC', '#E7DAD2', '#999999']\n",
    "sns.set_palette(colors)  # Set seaborn color palette\n",
    "\n",
    "# Calculate metrics\n",
    "knn_acc = accuracy_score(y_test, knn_predictions)\n",
    "rf_acc = accuracy_score(y_test, rf_predictions)\n",
    "gb_acc = accuracy_score(y_test, gb_predictions)\n",
    "nn_acc = accuracy_score(y_test, nn_predictions)\n",
    "ensemble_acc = accuracy_score(y_test, ensemble_predictions)\n",
    "\n",
    "knn_f1 = f1_score(y_test, knn_predictions)\n",
    "rf_f1 = f1_score(y_test, rf_predictions)\n",
    "gb_f1 = f1_score(y_test, gb_predictions)\n",
    "nn_f1 = f1_score(y_test, nn_predictions)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cross-validation Results Plot\n",
    "plt.figure()\n",
    "sns.lineplot(x=range(1, len(cv_scores) + 1), y=cv_scores, marker='o', markersize=8)\n",
    "plt.axhline(y=np.mean(cv_scores), color=colors[6], linestyle='--', \n",
    "            label=f'Mean CV Score: {np.mean(cv_scores):.4f}')\n",
    "plt.fill_between(range(1, len(cv_scores) + 1), \n",
    "                 np.mean(cv_scores) - np.std(cv_scores),\n",
    "                 np.mean(cv_scores) + np.std(cv_scores), \n",
    "                 color=colors[6], alpha=0.2)\n",
    "plt.xlabel('Cross-validation Fold')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Cross-validation Results')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('cv_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Architecture Flowchart\n",
    "plt.figure(figsize=(12, 8))\n",
    "G = nx.DiGraph()\n",
    "nodes = [\n",
    "    f'Input\\n({input_dim})', \n",
    "    f'Attention\\n({attention_dim})', \n",
    "    'Weighted\\nFeatures',\n",
    "    f'Hidden 1\\n({hidden_dim})', \n",
    "    f'Hidden 2\\n({hidden_dim})', \n",
    "    f'Output\\n({num_classes})'\n",
    "]\n",
    "positions = {\n",
    "    f'Input\\n({input_dim})': (0, 0.5),\n",
    "    f'Attention\\n({attention_dim})': (1, 1),\n",
    "    'Weighted\\nFeatures': (2, 0.5),\n",
    "    f'Hidden 1\\n({hidden_dim})': (3, 0.5),\n",
    "    f'Hidden 2\\n({hidden_dim})': (4, 0.5),\n",
    "    f'Output\\n({num_classes})': (5, 0.5)\n",
    "}\n",
    "\n",
    "for node in nodes:\n",
    "    G.add_node(node)\n",
    "\n",
    "edges = [\n",
    "    (f'Input\\n({input_dim})', f'Attention\\n({attention_dim})'),\n",
    "    (f'Input\\n({input_dim})', 'Weighted\\nFeatures'),\n",
    "    (f'Attention\\n({attention_dim})', 'Weighted\\nFeatures'),\n",
    "    ('Weighted\\nFeatures', f'Hidden 1\\n({hidden_dim})'),\n",
    "    (f'Hidden 1\\n({hidden_dim})', f'Hidden 2\\n({hidden_dim})'),\n",
    "    (f'Hidden 2\\n({hidden_dim})', f'Output\\n({num_classes})')\n",
    "]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Clear any existing seaborn styling for the network plot\n",
    "with plt.style.context('default'):\n",
    "    nx.draw(G, positions, with_labels=True, node_color=colors[1],\n",
    "            node_size=3000, font_size=10, font_weight='bold',\n",
    "            edge_color=colors[6], arrows=True, arrowsize=20)\n",
    "    plt.title('Model Architecture with Layer Dimensions')\n",
    "    plt.savefig('model_architecture.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Performance Comparison with F1-Scores\n",
    "# Calculate F1-scores\n",
    "knn_f1 = f1_score(y_test, knn_predictions)\n",
    "rf_f1 = f1_score(y_test, rf_predictions)\n",
    "gb_f1 = f1_score(y_test, gb_predictions)\n",
    "nn_f1 = f1_score(y_test, nn_predictions)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_predictions)\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Random Forest', 'Gradient Boosting', 'Neural Network', 'Ensemble'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, knn_predictions),\n",
    "        accuracy_score(y_test, rf_predictions),\n",
    "        accuracy_score(y_test, gb_predictions),\n",
    "        accuracy_score(y_test, nn_predictions),\n",
    "        accuracy_score(y_test, ensemble_predictions)\n",
    "    ],\n",
    "    'F1-Score': [knn_f1, rf_f1, gb_f1, nn_f1, ensemble_f1]\n",
    "})\n",
    "\n",
    "# Plot performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(performance_df['Model']))\n",
    "\n",
    "plt.bar(index - bar_width/2, performance_df['Accuracy'], bar_width, \n",
    "        label='Accuracy', color=colors[0])\n",
    "plt.bar(index + bar_width/2, performance_df['F1-Score'], bar_width,\n",
    "        label='F1-Score', color=colors[2])\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(index, performance_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create performance table\n",
    "performance_table = performance_df.style.format({\n",
    "    'Accuracy': '{:.4f}',\n",
    "    'F1-Score': '{:.4f}'\n",
    "}).to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quote\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "\n",
    "def create_flowchart_pdf():\n",
    "    # Create a new directed graph\n",
    "    dot = graphviz.Digraph('flowchart', \n",
    "                          format='pdf',\n",
    "                          engine='dot',\n",
    "                          graph_attr={'rankdir': 'TB', 'bgcolor': 'white'})\n",
    "    \n",
    "    # Define node styles\n",
    "    styles = {\n",
    "        'data': {'color': '#8ECFC9', 'penwidth': '3.0', 'fontcolor': '#8ECFC9', 'style': 'filled', 'fillcolor': 'white'},\n",
    "        'process': {'color': '#FFBE7A', 'penwidth': '3.0', 'fontcolor': '#FFBE7A', 'style': 'filled', 'fillcolor': 'white'},\n",
    "        'attention': {'color': '#FA7F6F', 'penwidth': '3.0', 'fontcolor': '#FA7F6F', 'style': 'filled', 'fillcolor': 'white'},\n",
    "        'layers': {'color': '#82B0D2', 'penwidth': '3.0', 'fontcolor': '#82B0D2', 'style': 'filled', 'fillcolor': 'white'},\n",
    "        'eval': {'color': '#BEB8DC', 'penwidth': '3.0', 'fontcolor': '#BEB8DC', 'style': 'filled', 'fillcolor': 'white'}\n",
    "    }\n",
    "    \n",
    "    # Add nodes\n",
    "    # Data Processing\n",
    "    with dot.subgraph(name='cluster_0') as s:\n",
    "        s.attr(label='Data Preprocessing', color='#8ECFC9')\n",
    "        for node in ['Wine Quality Dataset', 'Feature Engineering', 'Train-Test Split', 'StandardScaler', 'Convert to Tensors']:\n",
    "            s.node(node, **styles['data'])\n",
    "    \n",
    "    # Model Architecture\n",
    "    with dot.subgraph(name='cluster_1') as s:\n",
    "        s.attr(label='Attention Mechanism', color='#FA7F6F')\n",
    "        s.node('Attention Layer', **styles['attention'])\n",
    "        s.node('Multiply', shape='diamond', **styles['attention'])\n",
    "    \n",
    "    # Neural Network\n",
    "    with dot.subgraph(name='cluster_2') as s:\n",
    "        s.attr(label='Neural Network', color='#82B0D2')\n",
    "        for node in ['Input Layer', 'BatchNorm + Dropout', 'Hidden Layer 1 ReLU', 'Hidden Layer 2 ReLU', 'Output Layer']:\n",
    "            s.node(node, **styles['layers'])\n",
    "    \n",
    "    # Add edges\n",
    "    edges = [\n",
    "        ('Wine Quality Dataset', 'Feature Engineering'),\n",
    "        ('Feature Engineering', 'Train-Test Split'),\n",
    "        ('Train-Test Split', 'StandardScaler'),\n",
    "        ('StandardScaler', 'Convert to Tensors'),\n",
    "        ('Convert to Tensors', 'Input Layer'),\n",
    "        ('Input Layer', 'Attention Layer'),\n",
    "        ('Input Layer', 'Multiply'),\n",
    "        ('Attention Layer', 'Multiply'),\n",
    "        ('Multiply', 'BatchNorm + Dropout'),\n",
    "        ('BatchNorm + Dropout', 'Hidden Layer 1 ReLU'),\n",
    "        ('Hidden Layer 1 ReLU', 'Hidden Layer 2 ReLU'),\n",
    "        ('Hidden Layer 2 ReLU', 'Output Layer')\n",
    "    ]\n",
    "    \n",
    "    for edge in edges:\n",
    "        dot.edge(*edge)\n",
    "    \n",
    "    # Save the PDF\n",
    "    dot.render('flowchart', view=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_flowchart_pdf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
